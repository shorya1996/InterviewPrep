{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1be62e",
   "metadata": {},
   "source": [
    "# Interview preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869d688",
   "metadata": {},
   "source": [
    "# PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129cfc0",
   "metadata": {},
   "source": [
    "#1. Anargram Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9930f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anagram(string1, string2):\n",
    "    dict1 = {}\n",
    "    for i in string1:\n",
    "        if i in dict1:\n",
    "            dict1[i]+=1\n",
    "        else:\n",
    "            dict1[i]=1\n",
    "    for j in string2:\n",
    "        if j in dict1:\n",
    "            dict1[j]-=1\n",
    "        else:\n",
    "            dict1[j]=1\n",
    "    for k in dict1:\n",
    "        if dict1[k]!=0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86bc34e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anagram(\"god\", \"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5389",
   "metadata": {},
   "source": [
    "2.\n",
    "Get index (Competency tested: Scripting (any scripting language) )\n",
    "Given a list of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "Example:\n",
    "Given nums = [2, 7, 11, 15], target = 9,\n",
    "return [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78b2d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_sum(arr, k):\n",
    "    dict1={}\n",
    "    for i in range(0,len(arr)):\n",
    "        if arr[i] == k:\n",
    "            return i\n",
    "        elif len(dict1)==0:\n",
    "            dict1[arr[i]]=i\n",
    "        else:\n",
    "            for j in dict1:\n",
    "                if arr[i]+j==k:\n",
    "                    return [dict1[j],i]\n",
    "            dict1[arr[i]] = i\n",
    "    return \"No Value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c1510fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_sum([2, 7, 11, 15], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7655a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_sum([3,2,2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5de53121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "def two_sum(arr,target):\n",
    "    dict1 = dict()\n",
    "    for i in range(0,len(arr)):\n",
    "        if target-arr[i] in dict1:\n",
    "            return [dict1[target-arr[i]], i]\n",
    "        elif arr[i] not in dict1:\n",
    "            dict1[arr[i]] = i  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d67eb",
   "metadata": {},
   "source": [
    "3. Largest continuous Sum in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba0dcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_sum(arr):\n",
    "    maxm = arr[0]\n",
    "    current_sum = arr[0]\n",
    "    for i in range(1, len(arr)):\n",
    "        current_sum = max(arr[i], current_sum+arr[i])\n",
    "        maxm = max(current_sum, maxm)\n",
    "        print(current_sum, maxm)\n",
    "    return current_sum, maxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7ea99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 3\n",
      "2 3\n",
      "5 5\n",
      "1 5\n",
      "6 6\n",
      "12 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_sum([0,1,2,-1,3,-4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e211f1",
   "metadata": {},
   "source": [
    "given a string reverse all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad20073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(string):\n",
    "    return \" \".join(string.split(\" \")[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "234ff8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shorya am I Hi'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse(\"Hi I am shorya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe8490",
   "metadata": {},
   "source": [
    "String Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e900303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(string):\n",
    "    if len(string)==0:\n",
    "        return False\n",
    "    elif len(string)==1:\n",
    "        return string+\"1\"\n",
    "    else:\n",
    "        r=\"\"\n",
    "        i = 1\n",
    "        count=1\n",
    "        while i < len(string):\n",
    "            if string[i]==string[i-1]:\n",
    "                count+=1\n",
    "            else:\n",
    "                r = r+string[i-1]+str(count)\n",
    "                count=1\n",
    "            i+=1\n",
    "        r = r+string[i-1]+str(count)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c8b1633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A4B3C2'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress(\"AAAABBBCC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15121584",
   "metadata": {},
   "source": [
    "Balanced Paranthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc039e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_check(string):\n",
    "    if len(string)%2!=0:\n",
    "        return False\n",
    "    else:\n",
    "        is_open = set(['{','(', '['])\n",
    "        pair = set([('[',']'),('{','}'),('(',')')])\n",
    "        stack = []\n",
    "        for i in string:\n",
    "            if i in is_open:\n",
    "                stack.append(i)\n",
    "            else:\n",
    "                popped = stack.pop()\n",
    "                if (popped, i) not in pair:\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1027800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_check(\"[({()})]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd29b",
   "metadata": {},
   "source": [
    "#7. Longest Substring without repeating a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9f285a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_substring(string):\n",
    "    unique = list()\n",
    "    dict1 = {}\n",
    "    r = \"\"\n",
    "    for i in string:\n",
    "        if i not in unique:\n",
    "            unique.append(i)\n",
    "            r = r+i\n",
    "        else:\n",
    "            dict1[r]=len(r)\n",
    "            unique = []\n",
    "            unique.append(i)\n",
    "            r = i\n",
    "    dict1[r] = len(r)\n",
    "    maxm = 0\n",
    "    for j in dict1:\n",
    "        if dict1[j]>maxm:\n",
    "            maxm = dict1[j]\n",
    "            longest = j\n",
    "    return longest, maxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6a554669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abcdef', 6)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_substring(\"abcabcdef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2090b4",
   "metadata": {},
   "source": [
    "Given a nested list, return a flattened list with only the distinct numbers. The output should be a list ordered by the first occurrence of the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "54cacf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = {}\n",
    "def flatten_dedupe_list(s): \n",
    "    output = []\n",
    "    for x in s:\n",
    "        if isinstance(x, list):\n",
    "            s_sub = flatten_dedupe_list(x)\n",
    "            output.extend(s_sub)\n",
    "        else:\n",
    "            if x in num_dict.keys():\n",
    "                pass\n",
    "            else:\n",
    "                num_dict[x] = 1\n",
    "                output.append(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "de098845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_dedupe_list([[1,2,3], [2,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fb277c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest palindrome substring is: geeksskeeg\n",
      "Length is: 10\n"
     ]
    }
   ],
   "source": [
    "#9. longest palindromic substring\n",
    "def longestPalSubstr(string):\n",
    "    n = len(string) # calculating size of string\n",
    "    if (n < 2):\n",
    "        return n # if string is empty then size will be 0.\n",
    "    # if n==1 then, answer will be 1(single\n",
    "    # character will always palindrome)\n",
    "    start=0\n",
    "    maxLength = 1\n",
    "    for i in range(n):\n",
    "        low = i - 1\n",
    "        high = i + 1\n",
    "        while (high < n and string[high] == string[i] ):\t\t\t\t\t\t\t\n",
    "            high=high+1\n",
    "        while (low >= 0 and string[low] == string[i] ):\n",
    "            low=low-1\n",
    "\t\n",
    "        while (low >= 0 and high < n and string[low] == string[high] ):\n",
    "            low=low-1\n",
    "            high=high+1\n",
    "        length = high - low - 1\n",
    "        if (maxLength < length):\n",
    "            maxLength = length\n",
    "            start=low+1\n",
    "    print (\"Longest palindrome substring is:\",end=\" \")\n",
    "    print (string[start:start + maxLength])\n",
    "    return maxLength\n",
    "\t\n",
    "# Driver program to test above functions\n",
    "string = (\"forgeeksskeegfor\")\n",
    "print(\"Length is: \" + str(longestPalSubstr(string)))\n",
    "\n",
    "#This is code is contributed by saurabh yadav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "69b4552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Container with most water \n",
    "def maxArea(A, Len) :\n",
    "    area = 0\n",
    "    for i in range(Len) :\n",
    "        for j in range(i + 1, Len) :\n",
    "           \n",
    "            # Calculating the max area\n",
    "            area = max(area, min(A[j], A[i]) * (j - i))\n",
    "    return area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57e209",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "There are N people, numbered from 0 to N-1, playing a game. The K-th person is assigned the letter S[K].\n",
    "* At the beginning the 0th person sends a message, consisting of a single letter S[0], to the A[O]-th person.\n",
    "* When the K-th person receives the message, they append their letter S[K] to the message and forward it to A[K].\n",
    "* The game ends when the oth person receives the message. Find the final message.\n",
    "* You can assume that A contains every integer from 0 to N-1 exactly once.\n",
    "* Write a function: def solution(S, A) that given a string S and an array of\n",
    "* integers A, both of length N, returns a string denoting the final message received by the Oth person.\n",
    "\n",
    "*\n",
    "* Examples:\n",
    "* 1. Given S = \"cdeo\" and A = [3, 2, 0, 1], your function should returns \"code\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0c7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(S,A):\n",
    "    finalMessage = S[0]\n",
    "    destination = A[0]\n",
    "    while destination !=0:\n",
    "        finalMessage +=S[destination]\n",
    "        destination = A[destination]\n",
    "    return finalMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874795cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game(\"cdeo\", [3, 2, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a333fb",
   "metadata": {},
   "source": [
    "Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8153df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(inp_arr):\n",
    "    size = len(inp_arr)\n",
    "    if size > 1:\n",
    "        middle = size // 2\n",
    "        left_arr = inp_arr[:middle]\n",
    "        right_arr = inp_arr[middle:]\n",
    " \n",
    "        merge_sort(left_arr)\n",
    "        merge_sort(right_arr)\n",
    " \n",
    "        p = 0\n",
    "        q = 0\n",
    "        r = 0\n",
    " \n",
    "        left_size = len(left_arr)\n",
    "        right_size = len(right_arr)\n",
    "        while p < left_size and q < right_size:\n",
    "            if left_arr[p] < right_arr[q]:\n",
    "                inp_arr[r] = left_arr[p]\n",
    "                p += 1\n",
    "            else:\n",
    "                inp_arr[r] = right_arr[q]\n",
    "                q += 1\n",
    "            r += 1\n",
    "        while p < left_size:\n",
    "            inp_arr[r] = left_arr[p]\n",
    "            p += 1\n",
    "            r += 1\n",
    " \n",
    "        while q < right_size:\n",
    "            inp_arr[r]=right_arr[q]\n",
    "            q += 1\n",
    "            r += 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a4fb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 5, 7, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [1,2,0,5,8,7]\n",
    "merge_sort(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6617ec",
   "metadata": {},
   "source": [
    "Input: arr[] = {1, 2, 4, 6, 3, 7, 8}\n",
    "Output: 5\n",
    "Explanation: The missing number from 1 to 8 is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2c298fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(arr):\n",
    "    summ = 0\n",
    "    n=len(arr)\n",
    "    total_sum = (n + 1)*(n + 2)/2\n",
    "    for i in arr:\n",
    "        summ = summ+i\n",
    "    mis = total_sum-summ\n",
    "    return mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de7075fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing([1,2,4,6,3,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e69993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Local Maxima \n",
    "\n",
    "def localMaxima(arr, n)->list:\n",
    "    if len(arr)==0:\n",
    "        return []\n",
    "    elif len(arr)==1:\n",
    "        return [0]\n",
    "    else:\n",
    "        Maximum = []\n",
    "    if arr[0]>arr[1]:\n",
    "        Maximum.append(0)\n",
    "    for i in range(1, n-1):\n",
    "        if arr[i-1]<arr[i]>arr[i+1]:\n",
    "            Maximum.append(i)\n",
    "    if arr[-1] > arr[-2]:\n",
    "        Maximum.append(n-1)\n",
    "    if len(Maximum)>0:\n",
    "        return Maximum\n",
    "    else:\n",
    "        return False\n",
    "Arr = [1,2,5,3]\n",
    "N = len(Arr)\n",
    "print(localMaxima(Arr, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc49716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of a substring in a string\n",
    "def countFreq(pat, txt):\n",
    "    M = len(pat)\n",
    "    N = len(txt)\n",
    "    res = 0\n",
    "     \n",
    "    # A loop to slide pat[] one by one\n",
    "    for i in range(N - M + 1):\n",
    "         \n",
    "        # For current index i, check\n",
    "        # for pattern match\n",
    "        j = 0\n",
    "        while j < M:\n",
    "            if (txt[i + j] != pat[j]):\n",
    "                break\n",
    "            j += 1\n",
    " \n",
    "        if (j == M):\n",
    "            res += 1\n",
    "            j = 0\n",
    "    return res\n",
    "     \n",
    "# Driver Code\n",
    "if __name__ == '__main__':\n",
    "    txt = \"dhimanman\"\n",
    "    pat = \"man\"\n",
    "    print(countFreq(pat, txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e3501",
   "metadata": {},
   "source": [
    "# SPARK"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a5650a7",
   "metadata": {},
   "source": [
    "Create a dataframe with movie_id and price and ticket_id.\n",
    "find the total tickets sold for a movie \n",
    "group movies with same price\n",
    "\n",
    "movie_id price ticket_id\n",
    "1 500 123\n",
    "1 500 124\n",
    "2 650 400\n",
    "3 500 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "561dc827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark3'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark3'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark3'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ded00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Movie analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b71ec8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[1,500,123], [1,500,124], [2,650,400], [3,500,132]], [\"movie_id\", \"price\", \"ticket_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e375c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+\n",
      "|movie_id|price|ticket_id|\n",
      "+--------+-----+---------+\n",
      "|       1|  500|      123|\n",
      "|       1|  500|      124|\n",
      "|       2|  650|      400|\n",
      "|       3|  500|      132|\n",
      "+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e72fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_sold = df.groupBy(\"movie_id\").agg(count(\"movie_id\").alias(\"tickets_sold\")).orderBy([col(\"tickets_sold\").desc()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f449138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|movie_id|tickets_sold|\n",
      "+--------+------------+\n",
      "|       1|           2|\n",
      "|       2|           1|\n",
      "|       3|           1|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0384407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_with_same_price = df.groupBy(\"price\").agg(collect_list(\"movie_id\").alias(\"movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f7662da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|price|   movies|\n",
      "+-----+---------+\n",
      "|  650|      [2]|\n",
      "|  500|[1, 1, 3]|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies_with_same_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e635cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+---------+---------+\n",
      "|price|   movies|movie_id0|movie_id1|movie_id2|\n",
      "+-----+---------+---------+---------+---------+\n",
      "|  650|      [2]|        2|     null|     null|\n",
      "|  500|[1, 1, 3]|        1|        1|        3|\n",
      "+-----+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_of_columns = df_movies_with_same_price.select(size(col(\"movies\"))\\\n",
    "                                             .alias(\"array_length\")).agg({'array_length':'max'}).first()[0]\n",
    "column_list = []\n",
    "for i in range(no_of_columns):\n",
    "    col_name = \"movie_id\"+str(i)\n",
    "    df_movies_with_same_price = df_movies_with_same_price.withColumn(col_name, col(\"movies\")[i])\n",
    "df_movies_with_same_price.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1796b30",
   "metadata": {},
   "source": [
    "Consider and input text file, holding single row with pipe delimited as shown below. How will you \"apply line break to every 5th occurence of pipe delimiter\" and display as shown below ?\n",
    "\n",
    "Input text File :\n",
    "\n",
    "AZAR|BE|8|Bigdata|9273|Ramesh|Btech|3|Java|9274|Pratibhan|ME|6|DotNet|9275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e95d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark3'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark3'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Question1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"C:/Users/ShoryaSharma/Desktop/Question1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e0e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"chk\", regexp_replace(\"_c0\", \"(.*?\\\\|){5}\", \"$0-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17b87b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn(\"col_explode\", explode(split('chk', '\\|-'))).select('col_explode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ff0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.select(\"col_explode\").rdd.map(lambda x:x[0].split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ee55834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.toDF([\"Name\", \"Edu\", \"YearOfExp\", \"Tech\", \"Mobnum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8540e5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+-------+------+\n",
      "|     Name|  Edu|YearOfExp|   Tech|Mobnum|\n",
      "+---------+-----+---------+-------+------+\n",
      "|     AZAR|   BE|        8|Bigdata|  9273|\n",
      "|   Ramesh|Btech|        3|   Java|  9274|\n",
      "|Pratibhan|   ME|        6| DotNet|  9275|\n",
      "+---------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d4bc4",
   "metadata": {},
   "source": [
    "## handle_corrupted_record"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d422695",
   "metadata": {},
   "source": [
    "Modes in spark.read() \n",
    "\n",
    "mode - PERMISSIVE\n",
    "When we receive a corrupted record it puts the malformed record into a field.\n",
    "\n",
    "mode - FAILFAST\n",
    "When the spark program meets a corrupted record, this mode throws an exception and stops the spark job.\n",
    "\n",
    "mode - DROPMALFORMED\n",
    "When we specify the mode as DROPMALFORMED, the spark program will ignore the whole corrupted records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961afb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark3'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark3'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"handle_corrupted_record\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ac986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3483ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = spark.read.option(\"mode\", \"PERMISSIVE\").schema(schema).option(\"header\", True)\\\n",
    "                    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").csv(\"C:/Users/ShoryaSharma/Desktop/employee.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba469fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+--------------------+\n",
      "|EmployeeID|EmployeeName| Address|     _corrupt_record|\n",
      "+----------+------------+--------+--------------------+\n",
      "|      1001|       Floyd|   Delhi|                null|\n",
      "|      1002|      Watson|  Mumbai|1002,Watson,Mumba...|\n",
      "|      1003|       David|banglore|                null|\n",
      "|      1004|    Thompson| Chennai|                null|\n",
      "+----------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00e4c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|EmployeeID|EmployeeName| Address|\n",
      "+----------+------------+--------+\n",
      "|      1001|       Floyd|   Delhi|\n",
      "|      1003|       David|banglore|\n",
      "|      1004|    Thompson| Chennai|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "employee_df.where(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ee18ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|EmployeeID|EmployeeName|Address|\n",
      "+----------+------------+-------+\n",
      "|      1002|      Watson| Mumbai|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.where(col(\"_corrupt_record\").isNotNull()).drop(\"_corrupt_record\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae59bc",
   "metadata": {},
   "source": [
    "## Multi Delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766eecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|value               |\n",
      "+--------------------+\n",
      "|Name~|Age           |\n",
      "|Shorya,sharma~|25   |\n",
      "|Vini, Baishander~|23|\n",
      "|Aditi, Sharma~|26   |\n",
      "|Jatin, Shokeen~|24  |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"multi_delimiter\").getOrCreate()\n",
    "df = spark.read.text(\"C:/Users/ShoryaSharma/Desktop/multi.txt\")\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc1667ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name~|Age\n"
     ]
    }
   ],
   "source": [
    "header = df.first()[0]\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6cbebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Age']\n"
     ]
    }
   ],
   "source": [
    "schema = header.split('~|')\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "922408d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+\n",
      "|            Name|Age|\n",
      "+----------------+---+\n",
      "|            Name|Age|\n",
      "|   Shorya,sharma| 25|\n",
      "|Vini, Baishander| 23|\n",
      "|   Aditi, Sharma| 26|\n",
      "|  Jatin, Shokeen| 24|\n",
      "+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['value']!=\"header\").rdd.map(lambda x:x[0].split('~|')).toDF(schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e948a5",
   "metadata": {},
   "source": [
    "## Merge DataFrame in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ff29f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|shorya| 25|\n",
      "|  vini| 23|\n",
      "| jatin| 24|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"merge\").getOrCreate()\n",
    "df = spark.createDataFrame([[\"shorya\",\"25\"],[\"vini\",\"23\"], [\"jatin\", \"24\"]], [\"Name\", \"Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc8661b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Gender|\n",
      "+------+---+------+\n",
      "|Aakash| 25|     M|\n",
      "| Deepu| 23|     M|\n",
      "| Aditi| 24|     F|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[\"Aakash\",\"25\", \"M\"],[\"Deepu\",\"23\",\"M\"], [\"Aditi\", \"24\", \"F\"]], [\"Name\", \"Age\", \"Gender\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52314029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Gender|\n",
      "+------+---+------+\n",
      "|shorya| 25|  null|\n",
      "|  vini| 23|  null|\n",
      "| jatin| 24|  null|\n",
      "|Aakash| 25|     M|\n",
      "| Deepu| 23|     M|\n",
      "| Aditi| 24|     F|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first way\n",
    "from pyspark.sql.functions import lit\n",
    "df_add = df.withColumn(\"Gender\", lit(\"null\"))\n",
    "df_add.union(df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b3a89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Gender|\n",
      "+------+---+------+\n",
      "|Aakash| 25|     M|\n",
      "| Deepu| 23|     M|\n",
      "|  vini| 23|  null|\n",
      "| jatin| 24|  null|\n",
      "|shorya| 25|  null|\n",
      "| Aditi| 24|     F|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2nd way\n",
    "df_add = df.join(df1,on=[\"Name\", \"Age\"], how=\"outer\")\n",
    "df_add.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fb85dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|gender|  name|\n",
      "+---+------+------+\n",
      "| 25|  null|shorya|\n",
      "| 23|  null|  vini|\n",
      "| 24|  null| jatin|\n",
      "| 25|     M|Aakash|\n",
      "| 23|     M| Deepu|\n",
      "| 24|     F| Aditi|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best way\n",
    "def customUnion(df1, df2):\n",
    "    cols1 = [x.lower() for x in df1.columns]\n",
    "    cols2 = [x.lower() for x in df2.columns]\n",
    "    total_cols = sorted(cols1 + list(set(cols2) - set(cols1)))\n",
    "\n",
    "    def expr(mycols, allcols):\n",
    "        def processCols(colname):\n",
    "            if colname in mycols:\n",
    "                return colname\n",
    "            else:\n",
    "                return lit(None).alias(colname)\n",
    "\n",
    "        cols = map(processCols, allcols)\n",
    "        return list(cols)\n",
    "\n",
    "    appended = df1. \\\n",
    "        select(expr(cols1, total_cols)). \\\n",
    "        union(df2.select(expr(cols2, total_cols)))\n",
    "    return appended\n",
    "customUnion(df, df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b0034",
   "metadata": {},
   "source": [
    "## Speculative execution\n",
    "to handle the slow tasks in a stage due to environment issues like slow network, disk etc.\n",
    " \n",
    "spark.speculation >> false >> enables ( true ) or disables ( false ) speculative execution of tasks.\n",
    "\n",
    "spark.speculation.interval >> 100ms >> The time interval to use before checking for speculative tasks.\n",
    "\n",
    "spark.speculation.multiplier >> 1.5 >> How many times slower a task is than the median to be for speculation.\n",
    "\n",
    "spark.speculation.quantile >> 0.75 >> The percentage of tasks that has not finished yet at which to start speculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8fc30",
   "metadata": {},
   "source": [
    "## DATA SKEWNESS\n",
    "\n",
    "Techniques to handle in spark 2.0\n",
    "1. repartition \n",
    "2. Salting\n",
    "3. Reduce data skewness \n",
    "4. Bump up sparl.sql.autobroadcastJoinThreshold\n",
    "5. iterative broadcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f77b11",
   "metadata": {},
   "source": [
    "## PIVOT IN SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3679cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"merge\").getOrCreate()\n",
    "df = spark.createDataFrame([[\"1001\",\"English\", 84],[\"1001\",\"Science\",85], [\"1001\", \"Maths\", 86],\n",
    "                           [\"1002\",\"English\", 83],[\"1002\",\"Science\",85], [\"1002\", \"Maths\", 87]], [\"Roll\", \"Subject\", \"Marks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "760f3ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|Roll|Subject|Marks|\n",
      "+----+-------+-----+\n",
      "|1001|English|   84|\n",
      "|1001|Science|   85|\n",
      "|1001|  Maths|   86|\n",
      "|1002|English|   83|\n",
      "|1002|Science|   85|\n",
      "|1002|  Maths|   87|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af1ee65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-------+\n",
      "|Roll|English|Maths|Science|\n",
      "+----+-------+-----+-------+\n",
      "|1002|     83|   87|     85|\n",
      "|1001|     84|   86|     85|\n",
      "+----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tbl = df.groupBy(\"Roll\").pivot(\"Subject\").max(\"Marks\")\n",
    "tbl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e507a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
